{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSS Data Exploratory Analysis\n",
    "\n",
    "## Objective\n",
    "Explore the relationship between job satisfaction (SATJOB) and various psychological and work-related variables in the General Social Survey data.\n",
    "\n",
    "### Variables of Interest:\n",
    "- **Dependent Variable:** SATJOB (Job satisfaction)\n",
    "- **Independent Variables:** \n",
    "  - HLTHDEP (Health depression)\n",
    "  - FEELDOWN (Feeling down/depressed)\n",
    "  - NOINTEREST (No interest in activities)\n",
    "  - FEELNERV (Feeling nervous)\n",
    "  - WORRY (Worry levels)\n",
    "  - WRKMEANGFL (Work meaningfulness)\n",
    "  - RICHWORK (Rich work experiences)\n",
    "  - SATFIN (Financial satisfaction)\n",
    "  - DISCAFFWNV (Discrimination affected workplace environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure plotly to render inline in the notebook\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "The GSS data has already been extracted from R format to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GSS data directly from CSV\n",
    "print(\"Loading GSS data...\")\n",
    "\n",
    "# Load with pandas and appropriate NA handling\n",
    "gss_data = pd.read_csv(\n",
    "    'data/gss_subset.csv',\n",
    "    na_values=[\"NA\", \"\"],\n",
    "    on_bad_lines='skip',\n",
    "    low_memory=False\n",
    ")\n",
    "print(\"Loaded with pandas\")\n",
    "\n",
    "print(f\"Data loaded: {gss_data.shape[0]:,} rows, {gss_data.shape[1]} columns\")\n",
    "print(f\"\\nColumns: {list(gss_data.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(gss_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data info\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Shape: {gss_data.shape}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(gss_data.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "missing_counts = gss_data.isna().sum()\n",
    "print(missing_counts.to_frame().T)\n",
    "\n",
    "# Calculate missing percentages\n",
    "print(\"\\nMissing Data Analysis:\")\n",
    "print(\"=\"*50)\n",
    "total_rows = gss_data.shape[0]\n",
    "\n",
    "# Missing data analysis with pandas\n",
    "missing_data = (\n",
    "    pd.DataFrame({\n",
    "        'variable': gss_data.columns,\n",
    "        'missing_count': missing_counts.values\n",
    "    })\n",
    ")\n",
    "missing_data['total_rows'] = total_rows\n",
    "missing_data['available_count'] = missing_data['total_rows'] - missing_data['missing_count']\n",
    "missing_data['missing_pct'] = (missing_data['missing_count'] / missing_data['total_rows'] * 100).round(1)\n",
    "missing_data['availability_pct'] = (missing_data['available_count'] / missing_data['total_rows'] * 100).round(1)\n",
    "missing_data = missing_data.sort_values('missing_pct', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"{'Variable':<15} {'Missing':<8} {'Available':<10} {'Missing %':<10} {'Available %':<12}\")\n",
    "print(\"-\" * 65)\n",
    "for _, row in missing_data.iterrows():\n",
    "    print(f\"{row['variable']:<15} {int(row['missing_count']):<8} {int(row['available_count']):<10} {row['missing_pct']:<10} {row['availability_pct']:<12}\")\n",
    "\n",
    "# Summary statistics\n",
    "high_missing = (missing_data['missing_pct'] > 90).sum()\n",
    "med_missing = (missing_data['missing_pct'] > 50).sum()\n",
    "low_missing = (missing_data['missing_pct'] < 10).sum()\n",
    "complete_data = (missing_data['missing_pct'] == 0).sum()\n",
    "\n",
    "print(f\"\\nMissingness Summary:\")\n",
    "print(f\"- Variables with >90% missing: {high_missing}\")\n",
    "print(f\"- Variables with >50% missing: {med_missing}\")\n",
    "print(f\"- Variables with <10% missing: {low_missing}\")\n",
    "print(f\"- Variables with complete data: {complete_data}\")\n",
    "\n",
    "# Identify data quality tiers\n",
    "high_quality = missing_data.loc[missing_data['missing_pct'] < 10, 'variable'].tolist()\n",
    "medium_quality = missing_data.loc[(missing_data['missing_pct'] >= 10) & (missing_data['missing_pct'] < 50), 'variable'].tolist()\n",
    "low_quality = missing_data.loc[missing_data['missing_pct'] >= 50, 'variable'].tolist()\n",
    "\n",
    "print(f\"\\nData Quality Tiers:\")\n",
    "print(f\"High quality (<10% missing): {high_quality}\")\n",
    "print(f\"Medium quality (10-50% missing): {medium_quality}\")\n",
    "print(f\"Low quality (â‰¥50% missing): {low_quality}\")\n",
    "\n",
    "# Create visualization of missing data patterns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x=missing_data['variable'],\n",
    "    y=missing_data['missing_pct'],\n",
    "    name='Missing %',\n",
    "    marker_color='red'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Missing Data Percentage by Variable\",\n",
    "    xaxis_title=\"Variables\",\n",
    "    yaxis_title=\"Missing Percentage (%)\",\n",
    "    xaxis_tickangle=-45,\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on our variables of interest\n",
    "target_vars = ['satjob', 'hlthdep', 'feeldown', 'nointerest', 'feelnerv', 'worry',\n",
    "               'wrkmeangfl', 'richwork', 'satfin', 'discaffwnv']\n",
    "\n",
    "existing_vars = [var for var in target_vars if var in gss_data.columns]\n",
    "print(f\"Variables available for analysis: {existing_vars}\")\n",
    "\n",
    "# Create subset with our variables + some demographics\n",
    "analysis_vars = ['year', 'age', 'sex', 'race', 'degree'] + existing_vars\n",
    "available_analysis_vars = [var for var in analysis_vars if var in gss_data.columns]\n",
    "\n",
    "analysis_data = gss_data[available_analysis_vars]\n",
    "print(f\"\\nAnalysis dataset shape: {analysis_data.shape}\")\n",
    "print(f\"Variables: {list(analysis_data.columns)}\")\n",
    "\n",
    "# Analyze missingness patterns for our target variables\n",
    "print(f\"\\nMissingness Analysis for Target Variables:\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "target_missing = {}\n",
    "for var in existing_vars:\n",
    "    missing_count = gss_data[var].isna().sum()\n",
    "    total_count = gss_data.shape[0]\n",
    "    missing_pct = (missing_count / total_count * 100)\n",
    "    available_count = total_count - missing_count\n",
    "    \n",
    "    target_missing[var] = {\n",
    "        'missing_count': int(missing_count),\n",
    "        'available_count': int(available_count),\n",
    "        'missing_pct': float(missing_pct)\n",
    "    }\n",
    "    \n",
    "    print(f\"{var:<12}: {missing_count:>6} missing ({missing_pct:>5.1f}%), {available_count:>6} available\")\n",
    "\n",
    "# Check when these variables were measured (by year)\n",
    "print(f\"\\nVariable Availability by Year:\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "for var in existing_vars:\n",
    "    years_with_data = gss_data.loc[gss_data[var].notna(), 'year'].dropna().unique()\n",
    "    years_with_data = np.sort(years_with_data)\n",
    "    \n",
    "    if len(years_with_data) > 0:\n",
    "        year_range = f\"{years_with_data.min()}-{years_with_data.max()}\"\n",
    "        n_years = len(years_with_data)\n",
    "        print(f\"{var:<12}: {n_years:>2} years ({year_range})\")\n",
    "    else:\n",
    "        print(f\"{var:<12}: No data available\")\n",
    "\n",
    "# Identify which variables can be analyzed together\n",
    "print(f\"\\nVariable Overlap Analysis:\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "satjob_overlaps = {}\n",
    "for var in existing_vars:\n",
    "    if var != 'satjob':\n",
    "        overlap_count = (gss_data['satjob'].notna() & gss_data[var].notna()).sum()\n",
    "        satjob_overlaps[var] = int(overlap_count)\n",
    "        print(f\"SATJOB + {var:<12}: {overlap_count:>5} complete pairs\")\n",
    "\n",
    "# Find the best candidates for analysis\n",
    "viable_vars = [var for var, count in satjob_overlaps.items() if count > 1000]\n",
    "print(f\"\\nViable variables for correlation analysis (>1000 pairs): {viable_vars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependent Variable Analysis: Job Satisfaction (SATJOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze SATJOB\n",
    "print(\"Job Satisfaction (SATJOB) Analysis:\")\n",
    "\n",
    "# Use pandas for analysis\n",
    "sat = gss_data['satjob'].dropna()\n",
    "\n",
    "# Basic statistics using pandas\n",
    "stats = {\n",
    "    'count': float(sat.shape[0]),\n",
    "    'mean': sat.mean(),\n",
    "    'std': sat.std(),\n",
    "    'min': sat.min(),\n",
    "    '25%': sat.quantile(0.25),\n",
    "    'median': sat.median(),\n",
    "    '75%': sat.quantile(0.75),\n",
    "    'max': sat.max()\n",
    "}\n",
    "\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "for stat, value in stats.items():\n",
    "    print(f\"{stat}: {value:.6f}\")\n",
    "\n",
    "# Value counts using pandas\n",
    "print(f\"\\nValue Counts:\")\n",
    "vc = gss_data['satjob'].dropna().value_counts().sort_index()\n",
    "for level, count in vc.items():\n",
    "    print(f\"{level}: {count}\")\n",
    "\n",
    "# Visualization\n",
    "if not vc.empty:\n",
    "    fig = px.bar(x=vc.index.tolist(), \n",
    "                 y=vc.values.tolist(),\n",
    "                 title=\"Distribution of Job Satisfaction (SATJOB)\",\n",
    "                 labels={'x': 'Job Satisfaction Level', 'y': 'Count'})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Variables Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each independent variable\n",
    "independent_vars = ['hlthdep', 'feeldown', 'nointerest', 'feelnerv', 'worry',\n",
    "                   'wrkmeangfl', 'richwork', 'satfin', 'discaffwnv']\n",
    "\n",
    "available_indep_vars = [var for var in independent_vars if var in gss_data.columns]\n",
    "\n",
    "print(f\"Analyzing {len(available_indep_vars)} independent variables...\")\n",
    "\n",
    "print(\"\\nBasic Statistics for Independent Variables:\")\n",
    "for var in available_indep_vars:\n",
    "    print(f\"\\n{var.upper()}:\")\n",
    "    \n",
    "    # Series with non-null values\n",
    "    var_series = gss_data[var]\n",
    "    var_data = var_series.dropna()\n",
    "    \n",
    "    # Check if numeric or categorical\n",
    "    is_numeric = pd.api.types.is_numeric_dtype(var_series)\n",
    "    \n",
    "    if is_numeric and var_data.shape[0] > 0:\n",
    "        # Numeric statistics\n",
    "        print(f\"count: {int(var_data.shape[0])}\")\n",
    "        print(f\"mean: {var_data.mean()}\")\n",
    "        print(f\"std: {var_data.std()}\")\n",
    "        print(f\"min: {var_data.min()}\")\n",
    "        print(f\"max: {var_data.max()}\")\n",
    "    else:\n",
    "        # Categorical statistics\n",
    "        print(f\"count: {int(var_data.shape[0])}\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing_count = gss_data[var].isna().sum()\n",
    "    print(f\"Missing values: {int(missing_count)}\")\n",
    "    \n",
    "    # Show unique values and counts\n",
    "    if var_data.shape[0] > 0:\n",
    "        unique_vals = np.sort(var_data.unique())\n",
    "        if len(unique_vals) <= 10:\n",
    "            print(f\"Unique values: {unique_vals.tolist()}\")\n",
    "            \n",
    "            # Value counts\n",
    "            value_counts = var_data.value_counts().sort_index()\n",
    "            print(f\"Value counts:\")\n",
    "            for value, count in value_counts.items():\n",
    "                print(f\"{value}: {count}\")\n",
    "    else:\n",
    "        print(\"No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis using pandas\n",
    "print(\"Computing pairwise correlations with SATJOB...\")\n",
    "correlation_vars = ['satjob'] + available_indep_vars\n",
    "print(f\"Total observations: {gss_data.shape[0]}\")\n",
    "\n",
    "# Ensure SATJOB is numeric\n",
    "satjob_numeric = pd.to_numeric(gss_data['satjob'], errors='coerce')\n",
    "\n",
    "satjob_correlations = {}\n",
    "for var in available_indep_vars:\n",
    "    var_numeric = pd.to_numeric(gss_data[var], errors='coerce')\n",
    "    pair = pd.DataFrame({'satjob': satjob_numeric, var: var_numeric}).dropna(subset=['satjob', var])\n",
    "    n_obs = pair.shape[0]\n",
    "    if n_obs > 10:\n",
    "        corr_result = pair['satjob'].corr(pair[var])\n",
    "        satjob_correlations[var] = {\n",
    "            'correlation': float(corr_result),\n",
    "            'n_obs': int(n_obs)\n",
    "        }\n",
    "        print(f\"{var}: r = {corr_result:.3f} (n = {n_obs})\")\n",
    "    else:\n",
    "        print(f\"{var}: insufficient data (n = {n_obs})\")\n",
    "\n",
    "# Create visualization of correlations\n",
    "if satjob_correlations:\n",
    "    vars_list = list(satjob_correlations.keys())\n",
    "    corr_values = [satjob_correlations[var]['correlation'] for var in vars_list]\n",
    "    n_obs = [satjob_correlations[var]['n_obs'] for var in vars_list]\n",
    "\n",
    "    fig = px.bar(x=vars_list, y=corr_values,\n",
    "                 title=\"Correlations with Job Satisfaction (SATJOB)\",\n",
    "                 labels={'x': 'Variables', 'y': 'Correlation with SATJOB'},\n",
    "                 hover_data={'n_obs': n_obs})\n",
    "    fig.update_layout(xaxis_tickangle=-45)\n",
    "    fig.show()\n",
    "\n",
    "    print(\"\\nAttempting correlation matrix for variables with sufficient data...\")\n",
    "    vars_with_data = ['satjob'] + [var for var in available_indep_vars \n",
    "                                   if var in satjob_correlations and \n",
    "                                   satjob_correlations[var]['n_obs'] > 1000]\n",
    "    if len(vars_with_data) > 2:\n",
    "        df_corr = gss_data[vars_with_data].apply(pd.to_numeric, errors='coerce')\n",
    "        corr_matrix = df_corr.corr()\n",
    "        fig = px.imshow(corr_matrix.values,\n",
    "                        x=vars_with_data,\n",
    "                        y=vars_with_data,\n",
    "                        title=\"Correlation Matrix (Pairwise Complete Observations)\",\n",
    "                        color_continuous_scale='RdBu',\n",
    "                        aspect='auto',\n",
    "                        zmin=-1, zmax=1)\n",
    "        fig.show()\n",
    "        print(f\"\\nCorrelation matrix computed for {len(vars_with_data)} variables\")\n",
    "        print(\"Variables included:\", vars_with_data)\n",
    "    else:\n",
    "        print(\"Not enough variables with sufficient data for correlation matrix\")\n",
    "else:\n",
    "    print(\"No valid correlations could be computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for key relationships using pandas\n",
    "print(\"Creating relationship visualizations...\")\n",
    "\n",
    "# Create subplots for each independent variable vs SATJOB\n",
    "n_vars = len(available_indep_vars)\n",
    "n_cols = min(3, n_vars)\n",
    "n_rows = (n_vars + n_cols - 1) // n_cols\n",
    "\n",
    "fig = make_subplots(rows=n_rows, cols=n_cols,\n",
    "                    subplot_titles=[var.upper() for var in available_indep_vars])\n",
    "\n",
    "sat_series = pd.to_numeric(gss_data['satjob'], errors='coerce')\n",
    "\n",
    "for i, var in enumerate(available_indep_vars):\n",
    "    row = i // n_cols + 1\n",
    "    col = i % n_cols + 1\n",
    "    \n",
    "    # Clean data for this pair using pandas\n",
    "    var_series = pd.to_numeric(gss_data[var], errors='coerce')\n",
    "    mask = sat_series.notna() & var_series.notna()\n",
    "    if mask.any():\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=var_series[mask], y=sat_series[mask],\n",
    "                      mode='markers', name=var,\n",
    "                      showlegend=False, opacity=0.6),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "fig.update_layout(height=300*n_rows, \n",
    "                  title_text=\"Relationships between Independent Variables and Job Satisfaction\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics by Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group analysis by demographic variables using pandas\n",
    "demographic_vars = ['sex', 'race', 'degree']\n",
    "available_demo_vars = [var for var in demographic_vars if var in gss_data.columns]\n",
    "\n",
    "if len(available_demo_vars) > 0:\n",
    "    print(\"Job Satisfaction by Demographic Groups:\")\n",
    "    \n",
    "    for var in available_demo_vars:\n",
    "        print(f\"\\n{var.upper()}:\")\n",
    "        \n",
    "        # Calculate group statistics using pandas\n",
    "        df = gss_data[[var, 'satjob']].copy()\n",
    "        df[var] = df[var].astype('category')\n",
    "        df['satjob'] = pd.to_numeric(df['satjob'], errors='coerce')\n",
    "        df = df.dropna(subset=[var, 'satjob'])\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No data available\")\n",
    "            continue\n",
    "        \n",
    "        group_stats = df.groupby(var)['satjob'].agg(['count', 'mean', 'std']).reset_index()\n",
    "        group_stats['mean'] = group_stats['mean'].round(3)\n",
    "        group_stats['std'] = group_stats['std'].round(3)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"{'Group':<8} {'Count':<8} {'Mean':<8} {'Std':<8}\")\n",
    "        print(\"-\" * 32)\n",
    "        for _, row in group_stats.iterrows():\n",
    "            print(f\"{row[var]:<8} {int(row['count']):<8} {row['mean']:<8} {row['std']:<8}\")\n",
    "        \n",
    "        # Create box plot\n",
    "        fig = px.box(df, x=var, y='satjob', title=f\"Job Satisfaction by {var.upper()}\")\n",
    "        fig.update_xaxes(title=var.upper())\n",
    "        fig.update_yaxes(title=\"Job Satisfaction\")\n",
    "        fig.show()\n",
    "else:\n",
    "    print(\"No demographic variables available for group analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Trends Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze trends over time using pandas\n",
    "if 'year' in gss_data.columns:\n",
    "    print(\"Time Trends Analysis:\")\n",
    "    \n",
    "    # Prepare data\n",
    "    df = gss_data[['year', 'satjob']].copy()\n",
    "    df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "    df['satjob'] = pd.to_numeric(df['satjob'], errors='coerce')\n",
    "    df = df.dropna(subset=['year', 'satjob'])\n",
    "    \n",
    "    # Calculate mean satisfaction by year\n",
    "    yearly_means = df.groupby('year')['satjob'].agg(['count', 'mean', 'std']).reset_index()\n",
    "    yearly_means['mean'] = yearly_means['mean'].round(3)\n",
    "    yearly_means['std'] = yearly_means['std'].round(3)\n",
    "    yearly_means = yearly_means.sort_values('year')\n",
    "    \n",
    "    print(\"\\nJob Satisfaction by Year (first 10 years):\")\n",
    "    print(f\"{'Year':<8} {'Count':<8} {'Mean':<8} {'Std':<8}\")\n",
    "    print(\"-\" * 32)\n",
    "    for _, row in yearly_means.head(10).iterrows():\n",
    "        print(f\"{int(row['year']):<8} {int(row['count']):<8} {row['mean']:<8} {row['std']:<8}\")\n",
    "    \n",
    "    # Plot time trend\n",
    "    fig = px.line(yearly_means, x='year', y='mean',\n",
    "                  title=\"Average Job Satisfaction Over Time\",\n",
    "                  labels={'year': 'Year', 'mean': 'Average Job Satisfaction'})\n",
    "    fig.show()\n",
    "    \n",
    "    # Add trend analysis for key independent variables if available\n",
    "    if len(available_indep_vars) > 0:\n",
    "        print(\"\\nCorrelations with Year (time trends):\")\n",
    "        for var in available_indep_vars:\n",
    "            v = pd.to_numeric(gss_data[var], errors='coerce')\n",
    "            y = pd.to_numeric(gss_data['year'], errors='coerce')\n",
    "            mask = v.notna() & y.notna()\n",
    "            if mask.sum() > 10:\n",
    "                print(f\"{var}: {y[mask].corr(v[mask]):.3f}\")\n",
    "            else:\n",
    "                print(f\"{var}: insufficient data\")\n",
    "else:\n",
    "    print(\"Cannot perform time trends analysis - missing year variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings using pandas\n",
    "print(\"EXPLORATORY DATA ANALYSIS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\n1. Dataset Overview:\")\n",
    "print(f\"   - Total observations: {gss_data.shape[0]:,}\")\n",
    "print(f\"   - Total variables: {gss_data.shape[1]}\")\n",
    "print(f\"   - Variables of interest found: {len(existing_vars)}\")\n",
    "\n",
    "# Calculate SATJOB statistics using pandas\n",
    "sat_series = pd.to_numeric(gss_data['satjob'], errors='coerce')\n",
    "valid_count = sat_series.notna().sum()\n",
    "missing_count = sat_series.isna().sum()\n",
    "missing_pct = (missing_count / gss_data.shape[0]) * 100\n",
    "mean_val = sat_series.mean()\n",
    "std_val = sat_series.std()\n",
    "\n",
    "print(f\"\\n2. Dependent Variable (SATJOB):\")\n",
    "print(f\"   - Valid responses: {valid_count:,}\")\n",
    "print(f\"   - Missing values: {missing_count:,}\")\n",
    "print(f\"   - Missing percentage: {missing_pct:.1f}%\")\n",
    "print(f\"   - Mean satisfaction: {mean_val:.3f}\")\n",
    "print(f\"   - Standard deviation: {std_val:.3f}\")\n",
    "\n",
    "print(f\"\\n3. Missing Data Patterns:\")\n",
    "print(f\"   - Most psychological variables (HLTHDEP, FEELDOWN, etc.) have >95% missing data\")\n",
    "print(f\"   - These variables were likely asked only in specific survey years\")\n",
    "print(f\"   - SATFIN and RICHWORK have better coverage (~6% and ~61% missing respectively)\")\n",
    "print(f\"   - Complete case analysis across all variables is impossible (0 complete cases)\")\n",
    "\n",
    "print(f\"\\n4. Independent Variables:\")\n",
    "print(f\"   - Available variables: {', '.join(available_indep_vars)}\")\n",
    "if 'viable_vars' in locals():\n",
    "    print(f\"   - Variables with sufficient data for correlation: {viable_vars}\")\n",
    "\n",
    "# Check if we have correlation results\n",
    "if 'satjob_correlations' in locals() and satjob_correlations:\n",
    "    print(f\"\\n5. Key Correlations with SATJOB:\")\n",
    "    sorted_corrs = sorted(satjob_correlations.items(), \n",
    "                         key=lambda x: abs(x[1]['correlation']), \n",
    "                         reverse=True)\n",
    "    for var, corr_info in sorted_corrs[:5]:\n",
    "        print(f\"   - {var}: r = {corr_info['correlation']:.3f} (n = {corr_info['n_obs']})\")\n",
    "\n",
    "print(f\"\\n6. Data Quality Assessment:\")\n",
    "print(f\"   - Survey design: Different modules asked in different years\")\n",
    "print(f\"   - Temporal coverage: Data spans 1972-2024, but not all variables in all years\")\n",
    "print(f\"   - Analysis limitations: Pairwise correlations only, no multivariate analysis possible\")\n",
    "print(f\"   - Missing data mechanism: Likely Missing Completely At Random (MCAR) due to survey design\")\n",
    "\n",
    "print(f\"\\n7. Recommendations for Further Analysis:\")\n",
    "print(f\"   - Focus on variables with sufficient data (SATFIN, RICHWORK)\")\n",
    "print(f\"   - Analyze subsets of data by survey year or wave\")\n",
    "print(f\"   - Consider multiple imputation for key psychological variables\")\n",
    "print(f\"   - Investigate which years psychological measures were included\")\n",
    "print(f\"   - Use GSS panel data for longitudinal analysis of available variables\")\n",
    "print(f\"   - Consider external validation with other datasets\")\n",
    "\n",
    "print(f\"\\n8. Alternative Analysis Strategies:\")\n",
    "print(f\"   - Year-specific analysis for psychological variables\")\n",
    "print(f\"   - Propensity score matching to handle missing data\")\n",
    "print(f\"   - Sensitivity analysis with different missing data assumptions\")\n",
    "print(f\"   - Focus on demographic predictors which have better coverage\")\n",
    "\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
